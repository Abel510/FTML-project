{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression linéaire régularisée pour la prédiction\n",
    "\n",
    "Ce notebook compare différentes méthodes de régression linéaire régularisée pour prédire une variable cible à partir de caractéristiques données. Nous utiliserons Ridge, Lasso et ElasticNet pour trouver le meilleur modèle.\n",
    "\n",
    "## Chargement des données\n",
    "\n",
    "Nous chargeons les ensembles de données d'entraînement et de test à partir de fichiers NumPy (.npy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"C:\\\\Users\\\\abela\\\\Desktop\\\\Epita\\\\FTML\\\\Exo4\\\\X_test.npy\")\n",
    "y_train = np.load(\"C:\\\\Users\\\\abela\\\\Desktop\\\\Epita\\\\FTML\\\\Exo4\\\\y_test.npy\")\n",
    "X_test = np.load(\"C:\\\\Users\\\\abela\\\\Desktop\\\\Epita\\\\FTML\\\\Exo4\\\\X_train.npy\")\n",
    "y_test = np.load(\"C:\\\\Users\\\\abela\\\\Desktop\\\\Epita\\\\FTML\\\\Exo4\\\\y_train.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement des données\n",
    "\n",
    "Nous utilisons StandardScaler pour normaliser les caractéristiques, ce qui est important pour les algorithmes basés sur la régularisation. Nous aplatissons également les variables cibles pour assurer la compatibilité avec les modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation et évaluation\n",
    "\n",
    "Nous comparons trois algorithmes de régression régularisée :\n",
    "\n",
    "1. **Ridge**\n",
    "2. **Lasso** \n",
    "3. **ElasticNet** \n",
    "\n",
    "Nous évaluons chaque modèle en utilisant le score R² sur l'ensemble de test.\n",
    "\n",
    "Pour ElasticNet, nous effectuons également une recherche par grille (GridSearchCV) pour trouver les meilleurs hyperparamètres (alpha et l1_ratio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Test R² score: 0.5899\n",
      "Lasso Test R² score: 0.7573\n",
      "ElasticNet Test R² score: 0.8734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abela\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.717e-02, tolerance: 1.186e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\abela\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.437e-02, tolerance: 1.189e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ElasticNet parameters: {'alpha': 0.01, 'l1_ratio': 0.9}\n",
      "Best ElasticNet R² score: 0.9087183378852488\n",
      "Best ElasticNet Test R² score: 0.9347\n",
      "\n",
      "Résultats finaux:\n",
      "Ridge R² score: 0.5899\n",
      "Lasso R² score: 0.7573\n",
      "ElasticNet R² score: 0.8734\n",
      "Best ElasticNet R² score: 0.9347\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    print(f\"{model_name} Test R² score: {r2:.4f}\")\n",
    "    return r2\n",
    "\n",
    "models = {\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Lasso\": Lasso(alpha=0.1),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    results[name] = evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test, name)\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "elastic_grid = GridSearchCV(ElasticNet(), param_grid, cv=5, scoring='r2')\n",
    "elastic_grid.fit(X_train_scaled, y_train)\n",
    "print(\"Best ElasticNet parameters:\", elastic_grid.best_params_)\n",
    "print(\"Best ElasticNet R² score:\", elastic_grid.best_score_)\n",
    "\n",
    "best_elastic = elastic_grid.best_estimator_\n",
    "results[\"Best ElasticNet\"] = evaluate_model(best_elastic, X_train_scaled, X_test_scaled, y_train, y_test, \"Best ElasticNet\")\n",
    "\n",
    "print(\"\\nRésultats finaux:\")\n",
    "for name, r2 in results.items():\n",
    "    print(f\"{name} R² score: {r2:.4f}\")\n",
    "\n",
    "best_model = max(results, key=results.get)\n",
    "best_r2 = results[best_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats montrent les performances de chaque modèle, avec le modèle ElasticNet optimisé donnant généralement les meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conclusion\n",
      "1. Le meilleur modèle est Best ElasticNet\n",
      "2. Le meilleur score R² obtenu est 0.9347\n"
     ]
    }
   ],
   "source": [
    "print(\"1. Le meilleur modèle est\", best_model)\n",
    "print(f\"2. Le meilleur score R² obtenu est {best_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Nous identifions le meilleur modèle parmi ceux testés et rapportons son score R². Ce score indique la proportion de la variance dans la variable dépendante qui est prédictible à partir des variables indépendantes.\n",
    "\n",
    "Le modèle ElasticNet optimisé s'est avéré le plus performant, probablement en raison de sa capacité à combiner les avantages de la régularisation L1 et L2, s'adaptant ainsi mieux à la structure spécifique de nos données."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
