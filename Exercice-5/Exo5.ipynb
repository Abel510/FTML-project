{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"C:\\\\Users\\\\abela\\\\Desktop\\\\Epita\\\\FTML\\\\Exo5\\\\X_test.npy\")\n",
    "y_train = np.load(\"C:\\\\Users\\\\abela\\\\Desktop\\\\Epita\\\\FTML\\\\Exo5\\\\y_test.npy\")\n",
    "X_test = np.load(\"C:\\\\Users\\\\abela\\\\Desktop\\\\Epita\\\\FTML\\\\Exo5\\\\X_train.npy\")\n",
    "y_test = np.load(\"C:\\\\Users\\\\abela\\\\Desktop\\\\Epita\\\\FTML\\\\Exo5\\\\y_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (optimized) Test accuracy: 0.7460\n",
      "SVM (optimized) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.56      0.68       241\n",
      "           1       0.69      0.92      0.79       259\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.78      0.74      0.73       500\n",
      "weighted avg       0.78      0.75      0.74       500\n",
      "\n",
      "Random Forest (optimized) Test accuracy: 0.7900\n",
      "Random Forest (optimized) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.70      0.76       241\n",
      "           1       0.76      0.87      0.81       259\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.80      0.79      0.79       500\n",
      "weighted avg       0.80      0.79      0.79       500\n",
      "\n",
      "Gradient Boosting Test accuracy: 0.7580\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.68      0.73       241\n",
      "           1       0.73      0.83      0.78       259\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.76      0.76      0.76       500\n",
      "weighted avg       0.76      0.76      0.76       500\n",
      "\n",
      "XGBoost Test accuracy: 0.7720\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.70      0.75       241\n",
      "           1       0.75      0.84      0.79       259\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.78      0.77      0.77       500\n",
      "weighted avg       0.78      0.77      0.77       500\n",
      "\n",
      "Ensemble Test accuracy: 0.7920\n",
      "Ensemble Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.71      0.77       241\n",
      "           1       0.76      0.86      0.81       259\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.80      0.79      0.79       500\n",
      "weighted avg       0.80      0.79      0.79       500\n",
      "\n",
      "SVM (optimized) accuracy: 0.7460\n",
      "Random Forest (optimized) accuracy: 0.7900\n",
      "Gradient Boosting accuracy: 0.7580\n",
      "XGBoost accuracy: 0.7720\n",
      "Ensemble accuracy: 0.7920\n",
      "\n",
      "L'objectif de précision supérieure à 0.85 sur l'ensemble de test n'est pas atteint. Le meilleur score est 0.7920 pour le modèle Ensemble.\n",
      "\n",
      "Importance des caractéristiques:\n",
      "1 - Feature 16: 0.1397\n",
      "2 - Feature 26: 0.0922\n",
      "3 - Feature 18: 0.0647\n",
      "4 - Feature 12: 0.0580\n",
      "5 - Feature 15: 0.0532\n",
      "6 - Feature 13: 0.0532\n",
      "7 - Feature 4: 0.0518\n",
      "8 - Feature 2: 0.0388\n",
      "9 - Feature 24: 0.0297\n",
      "10 - Feature 3: 0.0255\n",
      "11 - Feature 6: 0.0248\n",
      "12 - Feature 10: 0.0248\n",
      "13 - Feature 9: 0.0229\n",
      "14 - Feature 7: 0.0214\n",
      "15 - Feature 19: 0.0208\n",
      "16 - Feature 14: 0.0208\n",
      "17 - Feature 25: 0.0205\n",
      "18 - Feature 11: 0.0203\n",
      "19 - Feature 8: 0.0202\n",
      "20 - Feature 5: 0.0196\n",
      "21 - Feature 20: 0.0194\n",
      "22 - Feature 21: 0.0193\n",
      "23 - Feature 27: 0.0189\n",
      "24 - Feature 0: 0.0189\n",
      "25 - Feature 23: 0.0184\n",
      "26 - Feature 28: 0.0181\n",
      "27 - Feature 29: 0.0167\n",
      "28 - Feature 17: 0.0166\n",
      "29 - Feature 22: 0.0155\n",
      "30 - Feature 1: 0.0153\n",
      "\n",
      "Conclusion:\n",
      "1. Le meilleur modèle est Ensemble\n",
      "2. La meilleure précision obtenue est 0.7920\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(f_classif, k='all')),\n",
    "])\n",
    "\n",
    "X_train_prepared = pipeline.fit_transform(X_train, y_train)\n",
    "X_test_prepared = pipeline.transform(X_test)\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Test accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{model_name} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return accuracy\n",
    "\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "svm = GridSearchCV(SVC(random_state=42), svm_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "svm_accuracy = evaluate_model(svm, X_train_prepared, X_test_prepared, y_train, y_test, \"SVM (optimized)\")\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rf = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "rf_accuracy = evaluate_model(rf, X_train_prepared, X_test_prepared, y_train, y_test, \"Random Forest (optimized)\")\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb_accuracy = evaluate_model(gb, X_train_prepared, X_test_prepared, y_train, y_test, \"Gradient Boosting\")\n",
    "\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "xgb_accuracy = evaluate_model(xgb, X_train_prepared, X_test_prepared, y_train, y_test, \"XGBoost\")\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm.best_estimator_),\n",
    "        ('rf', rf.best_estimator_),\n",
    "        ('gb', gb),\n",
    "        ('xgb', xgb)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "ensemble.estimators[0][1].probability = True\n",
    "\n",
    "ensemble_accuracy = evaluate_model(ensemble, X_train_prepared, X_test_prepared, y_train, y_test, \"Ensemble\")\n",
    "models = {\n",
    "    \"SVM (optimized)\": svm_accuracy,\n",
    "    \"Random Forest (optimized)\": rf_accuracy,\n",
    "    \"Gradient Boosting\": gb_accuracy,\n",
    "    \"XGBoost\": xgb_accuracy,\n",
    "    \"Ensemble\": ensemble_accuracy\n",
    "}\n",
    "\n",
    "for name, accuracy in models.items():\n",
    "    print(f\"{name} accuracy: {accuracy:.4f}\")\n",
    "\n",
    "best_model = max(models, key=models.get)\n",
    "best_accuracy = models[best_model]\n",
    "\n",
    "if best_accuracy > 0.85:\n",
    "    print(f\"\\nL'objectif de précision supérieure à 0.85 sur l'ensemble de test est atteint avec un score de {best_accuracy:.4f} pour le modèle {best_model}.\")\n",
    "else:\n",
    "    print(f\"\\nL'objectif de précision supérieure à 0.85 sur l'ensemble de test n'est pas atteint. Le meilleur score est {best_accuracy:.4f} pour le modèle {best_model}.\")\n",
    "\n",
    "if hasattr(rf.best_estimator_, 'feature_importances_'):\n",
    "    importances = rf.best_estimator_.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    print(\"\\nImportance des caractéristiques:\")\n",
    "    for f, idx in enumerate(indices):\n",
    "        print(\"{0} - Feature {1}: {2:.4f}\".format(f + 1, idx, importances[idx]))\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"1. Le meilleur modèle est\", best_model)\n",
    "print(f\"2. La meilleure précision obtenue est {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "[0 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0\n",
      " 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1\n",
      " 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 1 0\n",
      " 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0\n",
      " 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1\n",
      " 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1\n",
      " 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0\n",
      " 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 0\n",
      " 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0\n",
      " 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1\n",
      " 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
